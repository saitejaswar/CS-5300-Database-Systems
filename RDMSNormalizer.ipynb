{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T04:50:45.954616Z",
     "start_time": "2024-11-02T04:50:31.343541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import hashlib\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "\n",
    "class RDBMSNormalizer:\n",
    "    def __init__(self, csv_file, fd_file):\n",
    "        # Load input data\n",
    "        try:\n",
    "            self.data = pd.read_csv(csv_file)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File '{csv_file}' not found.\")\n",
    "            self.data = None\n",
    "            \n",
    "        with open('output.txt', 'w') as f:\n",
    "            f.write('')\n",
    "            f.close()\n",
    "            \n",
    "        self.primary_keys = []\n",
    "        self.candidate_keys = []\n",
    "        self.functional_dependencies = self.load_dependencies(fd_file)\n",
    "        self.functional_dependencies_dict = self.parse_dependencies(self.functional_dependencies)\n",
    "        self.normalized_tables = {}  # Store each normalized version of tables\n",
    "        self.updated_functional_dependencies = {}  # Store updated functional dependencies for each table\n",
    "        self.decomposition_log = []  # Track decompositions for dependencies\n",
    "        self.used_primary_keys = []  # Track primary keys being used\n",
    "        self.used_mvds = []  # Track MVDs being used\n",
    "        \n",
    "    def determine_highest_normal_form(self, table_data, primary_key, dependencies):\n",
    "        \"\"\"\n",
    "        Determines the highest normal form (1NF, 2NF, 3NF, BCNF, 4NF) for the given table.\n",
    "        Args:\n",
    "            table_data (DataFrame): The input table data as a pandas DataFrame.\n",
    "            primary_key (list): The primary key for the input table.\n",
    "            dependencies (list): A list of functional and multi-valued dependencies for the input table.\n",
    "        \"\"\"\n",
    "        print(f\"\\nChecking normal forms for the input table:\")\n",
    "        print(f\"Primary key: {primary_key}\")\n",
    "        print(f\"Dependencies: {dependencies}\")\n",
    "    \n",
    "        # Separate functional dependencies and multi-valued dependencies\n",
    "        functional_dependencies = []\n",
    "        mvds = []\n",
    "    \n",
    "        for dep in dependencies:\n",
    "            if '-->>' in dep:\n",
    "                determinant, dependent = dep.split('-->>')\n",
    "                mvds.append((determinant.strip(), dependent.strip()))\n",
    "            elif '->' in dep:\n",
    "                determinant, dependent = dep.split('->')\n",
    "                functional_dependencies.append((determinant.strip(), dependent.strip()))\n",
    "    \n",
    "        # Check for 1NF\n",
    "        if not self.is_in_1nf(table_data):\n",
    "            return \"Not in 1NF\"\n",
    "    \n",
    "        # Check for 2NF\n",
    "        if not self.is_in_2nf(functional_dependencies, primary_key):\n",
    "            return \"1NF\"\n",
    "    \n",
    "        # Check for 3NF\n",
    "        if not self.is_in_3nf(functional_dependencies, primary_key):\n",
    "            return \"2NF\"\n",
    "    \n",
    "        # Check for BCNF\n",
    "        if not self.is_in_bcnf(functional_dependencies, primary_key, table_data):\n",
    "            return \"3NF\"\n",
    "    \n",
    "        # Check for 4NF\n",
    "        if not self.is_in_4nf(mvds, functional_dependencies, primary_key, table_data):\n",
    "            return \"BCNF\"\n",
    "    \n",
    "        return \"4NF\"\n",
    "    \n",
    "    def is_in_1nf(self, table_data):\n",
    "        \"\"\"\n",
    "        Check if the table is in 1NF (no repeating groups or arrays).\n",
    "        Args:\n",
    "            table_data (DataFrame): The input table data as a pandas DataFrame.\n",
    "        \"\"\"\n",
    "        for column in table_data.columns:\n",
    "            if table_data[column].apply(lambda x: isinstance(x, list) or isinstance(x, dict) or ',' in str(x)).any():\n",
    "                print(f\"Repeating group found in column: {column}\")\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def is_in_2nf(self, functional_dependencies, primary_key):\n",
    "        \"\"\"\n",
    "        Check if the table is in 2NF (no partial dependencies).\n",
    "        \"\"\"\n",
    "        for determinant, dependent in functional_dependencies:\n",
    "            determinant_attrs = determinant.split(', ')\n",
    "            if self.is_partial_dependency(determinant_attrs, primary_key):\n",
    "                print(f\"Partial dependency found: {determinant} -> {dependent}\")\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def is_in_3nf(self, functional_dependencies, primary_key):\n",
    "        \"\"\"\n",
    "        Check if the table is in 3NF (no transitive dependencies).\n",
    "        \"\"\"\n",
    "        for determinant, dependent in functional_dependencies:\n",
    "            determinant_attrs = determinant.split(', ')\n",
    "            dependent_attrs = dependent.split(', ')\n",
    "            if self.is_transitive_dependency(determinant_attrs, dependent_attrs, primary_key):\n",
    "                print(f\"Transitive dependency found: {determinant} -> {dependent}\")\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def is_in_bcnf(self, functional_dependencies, primary_key, table_data):\n",
    "        \"\"\"\n",
    "        Check if the table is in BCNF (every determinant is a candidate key).\n",
    "        Args:\n",
    "            functional_dependencies (list): A list of functional dependencies for the table.\n",
    "            primary_key (list): The primary key of the table.\n",
    "            table_data (DataFrame): The input table data as a pandas DataFrame.\n",
    "        \"\"\"\n",
    "        for determinant, dependent in functional_dependencies:\n",
    "            determinant_attrs = determinant.split(', ')\n",
    "            if not self.is_superkey(determinant_attrs, table_data):\n",
    "                print(f\"Determinant {determinant} is not a superkey, violating BCNF.\")\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def is_in_4nf(self, mvds, functional_dependencies, primary_key, table_data):\n",
    "        \"\"\"\n",
    "        Check if the table is in 4NF (no non-trivial multivalued dependencies).\n",
    "        Args:\n",
    "            mvds (list): A list of multi-valued dependencies in the form of (determinant, dependent).\n",
    "            functional_dependencies (list): A list of functional dependencies for the table.\n",
    "            primary_key (list): The primary key of the table.\n",
    "            table_data (DataFrame): The input table data as a pandas DataFrame.\n",
    "        \"\"\"\n",
    "        for mvd in mvds:\n",
    "            determinant, dependent = mvd\n",
    "            determinant_attrs = determinant.split(', ')\n",
    "            dependent_attrs = dependent.split(', ')\n",
    "    \n",
    "            # Check if the MVD is non-trivial\n",
    "            if set(determinant_attrs) != set(primary_key):\n",
    "                print(f\"Non-trivial multivalued dependency detected: {determinant} -->> {dependent}\")\n",
    "                return False\n",
    "    \n",
    "        # Check for additional violations related to FDs and MVDs combined\n",
    "        for fd in functional_dependencies:\n",
    "            determinant, dependent = fd\n",
    "            determinant_attrs = determinant.split(', ')\n",
    "            dependent_attrs = dependent.split(', ')\n",
    "            \n",
    "            # If an FD violates 4NF conditions in combination with an MVD, identify the issue\n",
    "            for mvd in mvds:\n",
    "                mvd_determinant, mvd_dependent = mvd\n",
    "                if set(mvd_determinant.split(', ')) == set(determinant_attrs) and not set(dependent_attrs).issubset(set(primary_key)):\n",
    "                    print(f\"Violation of 4NF found: FD '{determinant} -> {dependent}' conflicts with MVD '{mvd_determinant} -->> {mvd_dependent}'\")\n",
    "                    return False\n",
    "    \n",
    "        return True\n",
    "    \n",
    "    def is_partial_dependency(self, determinant_attrs, primary_key):\n",
    "        \"\"\"\n",
    "        Determines if the given determinant is a partial dependency of the primary key.\n",
    "        A partial dependency occurs if a non-prime attribute is determined by part of a composite key.\n",
    "        \"\"\"\n",
    "        return set(determinant_attrs).issubset(set(primary_key)) and set(determinant_attrs) != set(primary_key)\n",
    "    \n",
    "    def is_transitive_dependency(self, determinant_attrs, dependent_attrs, primary_key):\n",
    "        \"\"\"\n",
    "        Determines if a given dependency is a transitive dependency.\n",
    "        \"\"\"\n",
    "        # If determinant is a subset of the primary key, it's not a transitive dependency\n",
    "        if set(determinant_attrs).issubset(set(primary_key)):\n",
    "            return False\n",
    "    \n",
    "        # Check if determinant is non-prime (not part of the primary key) and the dependent is also non-prime\n",
    "        is_non_prime = not set(determinant_attrs).issubset(set(primary_key))\n",
    "        has_non_key_dependents = any(attr not in primary_key for attr in dependent_attrs)\n",
    "    \n",
    "        return is_non_prime and has_non_key_dependents\n",
    "\n",
    "    def identify_mvd(self, functional_dependencies):\n",
    "        \"\"\"\n",
    "        Identifies multivalued dependencies (MVDs) from the functional dependencies.\n",
    "        Args:\n",
    "            functional_dependencies (list): A list of functional dependencies or multivalued dependencies.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of multivalued dependencies in the form (determinant, dependent).\n",
    "        \"\"\"\n",
    "        mvds = []\n",
    "        for fd in functional_dependencies:\n",
    "            # Check if the dependency is explicitly marked as an MVD (using -->> syntax)\n",
    "            if '-->>' in fd:\n",
    "                determinant, dependent = fd.split('-->>')\n",
    "                mvds.append((determinant.strip(), dependent.strip().split(', ')))\n",
    "        return mvds\n",
    "\n",
    "    def load_dependencies(self, file_path):\n",
    "        \"\"\"\n",
    "        Load functional and multivalued dependencies from the given file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                return [line.strip() for line in file.readlines()]\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File '{file_path}' not found.\")\n",
    "            return []\n",
    "\n",
    "    def split_multivalued_column(self, primary_keys, column_name):\n",
    "        \"\"\"\n",
    "        Splits a multi-valued column into separate rows for normalization purposes.\n",
    "        \"\"\"\n",
    "        if column_name not in self.data.columns:\n",
    "            print(f\"Error: Column '{column_name}' not found in data.\")\n",
    "            return self.data\n",
    "\n",
    "        # Create new rows based on multi-valued columns\n",
    "        rows = []\n",
    "        for _, row in self.data.iterrows():\n",
    "            values = row[column_name]\n",
    "            if pd.notna(values) and isinstance(values, str):\n",
    "                # Split multi-valued data by comma or other delimiters\n",
    "                for value in values.split(', '):\n",
    "                    new_row = row.copy()\n",
    "                    new_row[column_name] = value\n",
    "                    rows.append(new_row)\n",
    "            else:\n",
    "                rows.append(row)\n",
    "\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    def determine_minimal_candidate_keys(self, data):\n",
    "        \"\"\"\n",
    "        Determine minimal candidate keys for the given data.\n",
    "        \"\"\"\n",
    "        from itertools import combinations\n",
    "\n",
    "        columns = list(data.columns)\n",
    "        for r in range(1, len(columns) + 1):\n",
    "            for combination in combinations(columns, r):\n",
    "                if data[list(combination)].drop_duplicates().shape[0] == data.shape[0]:\n",
    "                    return list(combination)\n",
    "        return []\n",
    "\n",
    "    def normalize_1nf(self):\n",
    "        \"\"\"\n",
    "        Normalizes the data to First Normal Form (1NF) by removing multi-valued attributes.\n",
    "        \"\"\"\n",
    "        print(\"Normalizing to 1NF\")\n",
    "        primary_keys = self.primary_keys\n",
    "    \n",
    "        for column in self.data.columns:\n",
    "            if any(isinstance(value, str) and ', ' in str(value) for value in self.data[column]):\n",
    "                # Assume multi-valued attributes contain comma-separated values\n",
    "                new_table_df = self.split_multivalued_column(primary_keys, column)\n",
    "    \n",
    "                # Create new table with only the relevant primary keys and the multivalued column\n",
    "                relevant_columns = primary_keys + [column]\n",
    "                new_table_df = new_table_df[relevant_columns].drop_duplicates()\n",
    "    \n",
    "                # Store the new normalized table in self.normalized_tables\n",
    "                table_name = f\"{column}_1NF\"\n",
    "                self.normalized_tables[table_name] = {\n",
    "                    'data': new_table_df,\n",
    "                    'primary_key': primary_keys + [column]\n",
    "                }\n",
    "    \n",
    "        # Store the base table without multivalued attributes\n",
    "        non_multivalued_columns = [\n",
    "            col for col in self.data.columns if not any(isinstance(value, str) and ', ' in str(value) for value in self.data[col])\n",
    "        ]\n",
    "        self.normalized_tables['BaseTable_1NF'] = {\n",
    "            'data': self.data[non_multivalued_columns].drop_duplicates(),\n",
    "            'primary_key': primary_keys\n",
    "        }\n",
    "\n",
    "    def normalize_2nf(self):\n",
    "        \"\"\"\n",
    "        Normalizes the data to Second Normal Form (2NF).\n",
    "        \"\"\"\n",
    "        print(\"Normalizing to 2NF...\")\n",
    "        tables_to_process = list(self.normalized_tables.keys())\n",
    "    \n",
    "        while tables_to_process:\n",
    "            table_name = tables_to_process.pop(0)\n",
    "            table_data = self.normalized_tables[table_name]['data']\n",
    "            primary_key = self.normalized_tables[table_name]['primary_key']\n",
    "    \n",
    "            # Extract the updated functional dependencies from the current table structure\n",
    "            updated_dependencies = self.extract_dependencies(table_name, table_data, primary_key, self.functional_dependencies_dict['fd'])\n",
    "    \n",
    "            self.updated_functional_dependencies[table_name] = updated_dependencies\n",
    "            partial_dependencies = []\n",
    "    \n",
    "            print(f\"\\nProcessing table: {table_name}\")\n",
    "            print(f\"Primary key for {table_name}: {primary_key}\")\n",
    "            print(f\"Functional dependencies for {table_name}: {updated_dependencies}\")\n",
    "    \n",
    "            # Iterate through each functional dependency and check for partial dependencies\n",
    "            for fd in updated_dependencies:\n",
    "                determinant, dependent = fd.split('->')\n",
    "                determinant_attrs = determinant.strip().split(', ')\n",
    "                dependent_attrs = dependent.strip().split(', ')\n",
    "    \n",
    "                # Log which determinant and dependent attributes are being analyzed\n",
    "                print(f\"Analyzing functional dependency: {determinant_attrs} -> {dependent_attrs}\")\n",
    "                # Handle multivalued dependencies here if applicable\n",
    "                for mvd in self.functional_dependencies_dict['mvd']:\n",
    "                    if set(mvd[0].split(', ')).issubset(set(primary_key)):\n",
    "                        print(f\"Multivalued dependency found: {mvd[0]} -->> {mvd[1]}\")\n",
    "    \n",
    "                # Identify partial dependencies: determinant is a subset of the primary key but not the full primary key\n",
    "                if self.is_partial_dependency(determinant_attrs, primary_key):\n",
    "                    print(f\"Partial dependency detected: {determinant_attrs} -> {dependent_attrs}\")\n",
    "                    partial_dependencies.append((determinant_attrs, dependent_attrs))\n",
    "    \n",
    "                    # Create a new table for the partial dependency\n",
    "                    new_table_name = f\"{'_'.join(dependent_attrs)}_2NF\"\n",
    "                    new_table = table_data[determinant_attrs + dependent_attrs].drop_duplicates()\n",
    "    \n",
    "                    # Store the new table with the partial dependency removed\n",
    "                    self.normalized_tables[new_table_name] = {\n",
    "                        'data': new_table,\n",
    "                        'primary_key': determinant_attrs\n",
    "                    }\n",
    "    \n",
    "                    # Remove the dependent attributes from the original table\n",
    "                    table_data = table_data.drop(columns=dependent_attrs)\n",
    "                    \n",
    "                    # Update functional dependencies for the new table\n",
    "                    new_fd = f\"{', '.join(determinant_attrs)} -> {', '.join(dependent_attrs)}\"\n",
    "                    self.updated_functional_dependencies[new_table_name] = [new_fd]\n",
    "                    tables_to_process.append(new_table_name)\n",
    "                    print(f\"Partial Dependency found and decomposed: {new_fd}\")\n",
    "    \n",
    "            # Store the updated base table after removing partial dependencies\n",
    "            self.normalized_tables[table_name] = {\n",
    "                'data': table_data,\n",
    "                'primary_key': primary_key\n",
    "            }\n",
    "    \n",
    "            # Update functional dependencies for the base table\n",
    "            remaining_dependencies = [\n",
    "                fd for fd in updated_dependencies if fd not in [\n",
    "                    f\"{', '.join(d)} -> {', '.join(dep)}\" for d, dep in partial_dependencies\n",
    "                ]\n",
    "            ]\n",
    "            self.updated_functional_dependencies[table_name] = remaining_dependencies\n",
    "    \n",
    "            # Check if the table is already in 2NF\n",
    "            if not partial_dependencies:\n",
    "                print(f\"Table {table_name} is already in 2NF.\")\n",
    "            else:\n",
    "                print(f\"Table {table_name} has been decomposed to remove partial dependencies. Partial dependencies found: {partial_dependencies}\")\n",
    "\n",
    "    def normalize_3nf(self):\n",
    "        \"\"\"\n",
    "        Normalizes the data to Third Normal Form (3NF) by removing transitive dependencies.\n",
    "        \"\"\"\n",
    "        print(\"Normalizing to 3NF...\")\n",
    "        tables_to_process = list(self.normalized_tables.keys())\n",
    "\n",
    "        while tables_to_process:\n",
    "            table_name = tables_to_process.pop(0)\n",
    "            table_data = self.normalized_tables[table_name]['data']\n",
    "            primary_key = self.normalized_tables[table_name]['primary_key']\n",
    "\n",
    "            updated_dependencies = self.extract_dependencies(table_name, table_data, primary_key, self.functional_dependencies_dict['fd'])\n",
    "            self.updated_functional_dependencies[table_name] = updated_dependencies\n",
    "            transitive_dependencies = []\n",
    "\n",
    "            print(f\"\\nProcessing table: {table_name}\")\n",
    "            print(f\"Primary key for {table_name}: {primary_key}\")\n",
    "            print(f\"Functional dependencies for {table_name}: {updated_dependencies}\")\n",
    "\n",
    "            for fd in updated_dependencies:\n",
    "                determinant, dependent = fd.split('->')\n",
    "                determinant_attrs = determinant.strip().split(', ')\n",
    "                dependent_attrs = dependent.strip().split(', ')\n",
    "\n",
    "                print(f\"Analyzing functional dependency: {determinant_attrs} -> {dependent_attrs}\")\n",
    "\n",
    "                if self.is_transitive_dependency(determinant_attrs, dependent_attrs, primary_key):\n",
    "                    print(f\"Transitive dependency detected: {determinant_attrs} -> {dependent_attrs}\")\n",
    "                    transitive_dependencies.append((determinant_attrs, dependent_attrs))\n",
    "\n",
    "                    # Create a new table for the transitive dependency\n",
    "                    new_table_name = f\"{'_'.join(dependent_attrs)}_3NF\"\n",
    "                    new_table = table_data[determinant_attrs + dependent_attrs].drop_duplicates()\n",
    "\n",
    "                    self.normalized_tables[new_table_name] = {\n",
    "                        'data': new_table,\n",
    "                        'primary_key': determinant_attrs\n",
    "                    }\n",
    "\n",
    "                    table_data = table_data.drop(columns=dependent_attrs)\n",
    "\n",
    "                    new_fd = f\"{', '.join(determinant_attrs)} -> {', '.join(dependent_attrs)}\"\n",
    "                    self.updated_functional_dependencies[new_table_name] = [new_fd]\n",
    "                    tables_to_process.append(new_table_name)\n",
    "                    print(f\"Transitive Dependency found and decomposed: {new_fd}\")\n",
    "\n",
    "            self.normalized_tables[table_name] = {\n",
    "                'data': table_data,\n",
    "                'primary_key': primary_key\n",
    "            }\n",
    "\n",
    "            remaining_dependencies = [\n",
    "                fd for fd in updated_dependencies if fd not in [\n",
    "                    f\"{', '.join(d)} -> {', '.join(dep)}\" for d, dep in transitive_dependencies\n",
    "                ]\n",
    "            ]\n",
    "            self.updated_functional_dependencies[table_name] = remaining_dependencies\n",
    "\n",
    "            if not transitive_dependencies:\n",
    "                print(f\"Table {table_name} is already in 3NF.\")\n",
    "            else:\n",
    "                print(f\"Table {table_name} has been decomposed to remove transitive dependencies. Transitive dependencies found: {transitive_dependencies}\")\n",
    "    \n",
    "    def normalize_bcnf(self):\n",
    "        \"\"\"\n",
    "        Normalizes the data to Boyce-Codd Normal Form (BCNF).\n",
    "        \"\"\"\n",
    "        print(\"Normalizing to BCNF...\")\n",
    "        tables_to_process = list(self.normalized_tables.keys())\n",
    "\n",
    "        while tables_to_process:\n",
    "            table_name = tables_to_process.pop(0)\n",
    "            table_data = self.normalized_tables[table_name]['data']\n",
    "            primary_key = self.normalized_tables[table_name]['primary_key']\n",
    "\n",
    "            updated_dependencies = self.extract_dependencies(table_name, table_data, primary_key, self.functional_dependencies_dict['fd'])\n",
    "            self.updated_functional_dependencies[table_name] = updated_dependencies\n",
    "            bcnf_violations = []\n",
    "\n",
    "            print(f\"\\nProcessing table: {table_name}\")\n",
    "            print(f\"Primary key for {table_name}: {primary_key}\")\n",
    "            print(f\"Functional dependencies for {table_name}: {updated_dependencies}\")\n",
    "\n",
    "            # Identify BCNF violations: functional dependencies where the determinant is not a superkey\n",
    "            for fd in updated_dependencies:\n",
    "                determinant, dependent = fd.split('->')\n",
    "                determinant_attrs = determinant.strip().split(', ')\n",
    "                dependent_attrs = dependent.strip().split(', ')\n",
    "\n",
    "                print(f\"Analyzing functional dependency: {determinant_attrs} -> {dependent_attrs}\")\n",
    "\n",
    "                if not self.is_superkey(determinant_attrs, table_data):\n",
    "                    print(f\"BCNF violation detected: {determinant_attrs} -> {dependent_attrs}\")\n",
    "                    bcnf_violations.append((determinant_attrs, dependent_attrs))\n",
    "\n",
    "                    # Create a new table for the violating dependency\n",
    "                    new_table_name = f\"{'_'.join(dependent_attrs)}_BCNF\"\n",
    "                    new_table = table_data[determinant_attrs + dependent_attrs].drop_duplicates()\n",
    "\n",
    "                    # Store the new table with the violating dependency removed\n",
    "                    self.normalized_tables[new_table_name] = {\n",
    "                        'data': new_table,\n",
    "                        'primary_key': determinant_attrs\n",
    "                    }\n",
    "\n",
    "                    # Remove the dependent attributes from the original table\n",
    "                    table_data = table_data.drop(columns=dependent_attrs)\n",
    "\n",
    "                    # Update functional dependencies for the new table\n",
    "                    new_fd = f\"{', '.join(determinant_attrs)} -> {', '.join(dependent_attrs)}\"\n",
    "                    self.updated_functional_dependencies[new_table_name] = [new_fd]\n",
    "                    tables_to_process.append(new_table_name)\n",
    "                    print(f\"BCNF violation found and decomposed: {new_fd}\")\n",
    "\n",
    "            # Store the updated base table after removing BCNF violations\n",
    "            self.normalized_tables[table_name] = {\n",
    "                'data': table_data,\n",
    "                'primary_key': primary_key\n",
    "            }\n",
    "\n",
    "            # Update functional dependencies for the base table\n",
    "            remaining_dependencies = [\n",
    "                fd for fd in updated_dependencies if fd not in [\n",
    "                    f\"{', '.join(d)} -> {', '.join(dep)}\" for d, dep in bcnf_violations\n",
    "                ]\n",
    "            ]\n",
    "            self.updated_functional_dependencies[table_name] = remaining_dependencies\n",
    "\n",
    "            if not bcnf_violations:\n",
    "                print(f\"Table {table_name} is already in BCNF.\")\n",
    "            else:\n",
    "                print(f\"Table {table_name} has been decomposed to remove BCNF violations. Violations found: {bcnf_violations}\")\n",
    "    \n",
    "    def normalize_4nf(self):\n",
    "        \"\"\"\n",
    "        Normalize the table to 4NF by decomposing it based on multivalued dependencies (MVDs).\n",
    "        Create separate relations for each MVD until no non-trivial MVDs remain.\n",
    "        \"\"\"\n",
    "        print(\"Normalizing to 4NF...\")\n",
    "        tables_to_process = [{'data': self.data, 'table_name': 'BaseTable'}]\n",
    "        decomposed_tables = []\n",
    "    \n",
    "        seen_hashes = set()\n",
    "    \n",
    "        while tables_to_process:\n",
    "            table = tables_to_process.pop(0)\n",
    "            table_data = table['data']\n",
    "            table_name = table['table_name']\n",
    "    \n",
    "            # Generate a hash for the current table to track it\n",
    "            table_hash = hashlib.md5(pd.util.hash_pandas_object(table_data, index=True).values).hexdigest()\n",
    "    \n",
    "            # If the table has already been processed, skip it\n",
    "            if table_hash in seen_hashes:\n",
    "                continue\n",
    "            seen_hashes.add(table_hash)\n",
    "    \n",
    "            # Identify MVDs in the current table\n",
    "            mvds = self.identify_multivalued_dependencies(table_data)\n",
    "    \n",
    "            if not mvds:\n",
    "                # No MVDs found, add the table to final decomposed tables\n",
    "                self.normalized_tables[table_name] = {\n",
    "                    'data': table_data,\n",
    "                    'primary_key': self.determine_primary_key(table_data)\n",
    "                }\n",
    "                decomposed_tables.append(table_data)\n",
    "                continue\n",
    "    \n",
    "            # Decompose based on the identified MVDs\n",
    "            for mvd in mvds:\n",
    "                determinant, dependent = mvd\n",
    "    \n",
    "                # Ensure determinant and dependent are treated as lists\n",
    "                if isinstance(determinant, str):\n",
    "                    determinant = [determinant]\n",
    "                if isinstance(dependent, str):\n",
    "                    dependent = [dependent]\n",
    "    \n",
    "                print(f\"Decomposing table {table_name} based on MVD: {determinant} -->> {dependent}\")\n",
    "    \n",
    "                # Decompose the table into two relations as per the MVD\n",
    "                relation_1 = table_data[determinant + dependent].drop_duplicates().reset_index(drop=True)\n",
    "                relation_2 = table_data[determinant + [col for col in table_data.columns if col not in dependent]].drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "                # Assign names to the new relations\n",
    "                relation_1_name = f\"{table_name}_{'_'.join(determinant)}_dependent_4NF\"\n",
    "                relation_2_name = f\"{table_name}_{'_'.join(determinant)}_remaining_4NF\"\n",
    "    \n",
    "                # Track the new decomposed tables if they are unique\n",
    "                for relation, name in zip([relation_1, relation_2], [relation_1_name, relation_2_name]):\n",
    "                    relation_hash = hashlib.md5(pd.util.hash_pandas_object(relation, index=True).values).hexdigest()\n",
    "                    if relation_hash not in seen_hashes:\n",
    "                        seen_hashes.add(relation_hash)\n",
    "                        self.normalized_tables[name] = {\n",
    "                            'data': relation,\n",
    "                            'primary_key': self.determine_primary_key(relation)\n",
    "                        }\n",
    "                        decomposed_tables.append(relation)\n",
    "                        tables_to_process.append({'data': relation, 'table_name': name})\n",
    "    \n",
    "        return decomposed_tables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def identify_multivalued_dependencies(self, table_data):\n",
    "        \"\"\"\n",
    "        Identifies non-trivial multivalued dependencies in the table.\n",
    "        Args:\n",
    "            table_data (DataFrame): The current table data as a pandas DataFrame.\n",
    "        Returns:\n",
    "            list: A list of multivalued dependencies that exist in the table.\n",
    "        \"\"\"\n",
    "        mvds = []\n",
    "    \n",
    "        for mvd in self.functional_dependencies_dict['mvd']:\n",
    "            determinant, dependent = mvd\n",
    "            determinant = determinant.split(\", \")\n",
    "            dependent = dependent.split(\", \")\n",
    "    \n",
    "            # Check if the determinant and dependent columns exist in the table data\n",
    "            if not all(col in table_data.columns for col in determinant + dependent):\n",
    "                continue  # Skip this MVD if columns are missing\n",
    "    \n",
    "            # Check if the determinant values are independent of the dependent values\n",
    "            try:\n",
    "                if not table_data.groupby(determinant)[dependent].nunique().eq(1).all().all():\n",
    "                    mvds.append(mvd)\n",
    "            except KeyError as e:\n",
    "                print(f\"Warning: {e}\")\n",
    "                continue\n",
    "    \n",
    "        return mvds\n",
    "    \n",
    "    def decompose_based_on_mvd(self, table_data, determinant, dependent):\n",
    "        \"\"\"\n",
    "        Decomposes the table based on the given multivalued dependency.\n",
    "        Args:\n",
    "            table_data (DataFrame): The current table data as a pandas DataFrame.\n",
    "            determinant (list): The determinant attributes of the MVD.\n",
    "            dependent (list): The dependent attributes of the MVD.\n",
    "        Returns:\n",
    "            list: A list of DataFrames resulting from the decomposition.\n",
    "        \"\"\"\n",
    "        # Ensure determinant and dependent are lists\n",
    "        if isinstance(determinant, str):\n",
    "            determinant = [determinant]\n",
    "        if isinstance(dependent, str):\n",
    "            dependent = [dependent]\n",
    "    \n",
    "        remaining_columns = [col for col in table_data.columns if col not in dependent]\n",
    "    \n",
    "        # Create new relations based on decomposition\n",
    "        try:\n",
    "            relation_1 = table_data[determinant + dependent].drop_duplicates().reset_index(drop=True)\n",
    "            relation_2 = table_data[remaining_columns].drop_duplicates().reset_index(drop=True)\n",
    "        except KeyError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return [table_data]\n",
    "    \n",
    "        return [relation_1, relation_2]\n",
    "\n",
    "\n",
    "    def determine_primary_key(self, table_data):\n",
    "        \"\"\"\n",
    "        Determine minimal candidate keys for the given data.\n",
    "        Args:\n",
    "            table_data (DataFrame): The data for which to determine the primary key.\n",
    "        Returns:\n",
    "            list: A list of columns that form the primary key.\n",
    "        \"\"\"\n",
    "        from itertools import combinations\n",
    "\n",
    "        columns = list(table_data.columns)\n",
    "        for r in range(1, len(columns) + 1):\n",
    "            for combination in combinations(columns, r):\n",
    "                if table_data[list(combination)].drop_duplicates().shape[0] == table_data.shape[0]:\n",
    "                    return list(combination)\n",
    "        return []\n",
    "\n",
    "    def identify_remaining_mvds(self, table_data, determinant_attrs):\n",
    "        \"\"\"\n",
    "        Identifies any remaining MVDs in the table after decomposition.\n",
    "        Args:\n",
    "            table_data (DataFrame): The current table data as a pandas DataFrame.\n",
    "            determinant_attrs (list): The determinant attributes of the MVD.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of remaining MVDs in the form (determinant, dependent).\n",
    "        \"\"\"\n",
    "        remaining_mvds = []\n",
    "        for mvd in self.functional_dependencies_dict['mvd']:\n",
    "            determinant, dependent = mvd\n",
    "            if set(determinant.split(', ')).issubset(set(table_data.columns)):\n",
    "                remaining_mvds.append(mvd)\n",
    "        return remaining_mvds\n",
    "    \n",
    "    def normalize_5nf(self):\n",
    "        print(\"Normalizing to 5NF...\")\n",
    "        tables_to_process = [{'data': self.data, 'table_name': 'BaseTable'}]\n",
    "        decomposed_tables = []\n",
    "\n",
    "        while tables_to_process:\n",
    "            table = tables_to_process.pop(0)\n",
    "            table_data = table['data']\n",
    "            table_name = table['table_name']\n",
    "\n",
    "            join_dependencies = self.identify_join_dependencies(table_data)\n",
    "            while join_dependencies:\n",
    "                jd = join_dependencies.pop(0)\n",
    "                print(f\"Decomposing table {table_name} based on join dependency: {jd}\")\n",
    "                relations = self.decompose_based_on_join_dependency(table_data, jd)\n",
    "\n",
    "                for i, relation in enumerate(relations):\n",
    "                    new_table_name = f\"{table_name}_JD_{i + 1}_5NF\"\n",
    "                    self.normalized_tables[new_table_name] = {\n",
    "                        'data': relation,\n",
    "                        'primary_key': self.determine_primary_key(relation)\n",
    "                    }\n",
    "                    decomposed_tables.append(relation)\n",
    "                    tables_to_process.append({'data': relation, 'table_name': new_table_name})\n",
    "\n",
    "            if not join_dependencies:\n",
    "                self.normalized_tables[table_name] = {\n",
    "                    'data': table_data,\n",
    "                    'primary_key': self.determine_primary_key(table_data)\n",
    "                }\n",
    "                decomposed_tables.append(table_data)\n",
    "\n",
    "        return decomposed_tables\n",
    "\n",
    "    def identify_join_dependencies(self, table_data):\n",
    "        \"\"\"\n",
    "        Identifies non-trivial join dependencies in the table.\n",
    "        Args:\n",
    "            table_data (DataFrame): The current table data as a pandas DataFrame.\n",
    "        Returns:\n",
    "            list: A list of join dependencies that exist in the table.\n",
    "        \"\"\"\n",
    "        join_dependencies = []\n",
    "\n",
    "        columns = list(table_data.columns)\n",
    "        for r in range(1, len(columns)):\n",
    "            for combination in combinations(columns, r):\n",
    "                remaining_columns = [col for col in columns if col not in combination]\n",
    "\n",
    "                # Check if splitting into two sets yields tables that can be naturally joined to reconstruct the original table\n",
    "                left_df = table_data[list(combination)].drop_duplicates()\n",
    "                right_df = table_data[remaining_columns].drop_duplicates()\n",
    "\n",
    "                # Perform a natural join on the split tables\n",
    "                reconstructed_df = pd.merge(left_df, right_df, how='inner')\n",
    "\n",
    "                # If the join yields the same number of rows as the original table, we have a join dependency\n",
    "                if reconstructed_df.shape[0] == table_data.shape[0]:\n",
    "                    join_dependencies.append((list(combination), remaining_columns))\n",
    "\n",
    "        return join_dependencies\n",
    "\n",
    "    def decompose_based_on_join_dependency(self, table_data, join_dependency):\n",
    "        \"\"\"\n",
    "        Decomposes the table based on the given join dependency.\n",
    "        Args:\n",
    "            table_data (DataFrame): The current table data as a pandas DataFrame.\n",
    "            join_dependency: The join dependency to decompose on.\n",
    "        Returns:\n",
    "            list: A list of DataFrames resulting from the decomposition.\n",
    "        \"\"\"\n",
    "        left_columns, right_columns = join_dependency\n",
    "        left_table = table_data[left_columns].drop_duplicates().reset_index(drop=True)\n",
    "        right_table = table_data[right_columns].drop_duplicates().reset_index(drop=True)\n",
    "        return [left_table, right_table]\n",
    "    \n",
    "    def is_superkey(self, determinant_attrs, table_data):\n",
    "        \"\"\"\n",
    "        Helper function to check if the given determinant attributes form a superkey.\n",
    "        A superkey uniquely identifies each row in the table.\n",
    "        \"\"\"\n",
    "        print(f\"Checking if {determinant_attrs} is a superkey.\")\n",
    "        # If determinant attributes uniquely identify rows in the table, it is a superkey\n",
    "        if table_data[determinant_attrs].drop_duplicates().shape[0] == table_data.shape[0]:\n",
    "            print(f\"{determinant_attrs} is a superkey.\")\n",
    "            return True\n",
    "        print(f\"{determinant_attrs} is not a superkey.\")\n",
    "        return False\n",
    "\n",
    "    def parse_dependencies(self, dependencies):\n",
    "        \"\"\"\n",
    "        Parse the loaded dependencies to separate functional and multivalued dependencies.\n",
    "        \"\"\"\n",
    "        fd_dict = {'fd': [], 'mvd': []}\n",
    "        for dep in dependencies:\n",
    "            if '-->>' in dep:\n",
    "                determinant, dependent = dep.split('-->>')\n",
    "                fd_dict['mvd'].append((determinant.strip().replace('{', '').replace('}', ''),\n",
    "                                       dependent.strip().replace('{', '').replace('}', '')))\n",
    "            elif '-->' in dep:\n",
    "                determinant, dependent = dep.split('-->')\n",
    "                fd_dict['fd'].append((determinant.strip().replace('{', '').replace('}', ''),\n",
    "                                      dependent.strip().replace('{', '').replace('}', '')))\n",
    "        return fd_dict\n",
    "\n",
    "    def extract_dependencies(self, table_name, table_data, primary_key, functional_dependencies):\n",
    "        \"\"\"\n",
    "        Extract functional dependencies for the given table based on the provided dependencies.\n",
    "        Enhanced to check if the determinant uniquely identifies the dependent attributes.\n",
    "        \"\"\"\n",
    "        extracted_dependencies = []\n",
    "        for determinant, dependent in functional_dependencies:\n",
    "            determinant_attrs = determinant.split(', ')\n",
    "            dependent_attrs = dependent.split(', ')\n",
    "\n",
    "            # Validate that all determinant and dependent attributes exist in the table columns\n",
    "            if all(attr in table_data.columns for attr in determinant_attrs + dependent_attrs):\n",
    "                # Check if determinant attributes are present in the table\n",
    "                if set(determinant_attrs).issubset(set(table_data.columns)):\n",
    "                    # Check if determinant uniquely identifies the dependent\n",
    "                    if table_data[determinant_attrs].drop_duplicates().shape[0] == table_data.drop_duplicates(subset=determinant_attrs + dependent_attrs).shape[0]:\n",
    "                        extracted_dependencies.append(f\"{determinant} -> {dependent}\")\n",
    "            # else:\n",
    "                # print(f\"Warning: Dependency '{determinant} -> {dependent}' contains attributes not in table '{table_name}'\")\n",
    "        return extracted_dependencies\n",
    "\n",
    "    def is_partial_dependency(self, determinant_attrs, primary_key):\n",
    "        \"\"\"\n",
    "        Helper function to check if the given determinant attributes form a partial dependency.\n",
    "        A partial dependency exists if:\n",
    "        1. The determinant is a proper subset of the primary key, or\n",
    "        2. The determinant is a proper subset of any candidate key.\n",
    "        \"\"\"\n",
    "        print(f\"Checking if {determinant_attrs} is a partial dependency of primary key {primary_key}\")\n",
    "    \n",
    "        # Check if the determinant is a proper subset of the primary key\n",
    "        if set(determinant_attrs).issubset(set(primary_key)) and set(determinant_attrs) != set(primary_key):\n",
    "            print(f\"{determinant_attrs} is a proper subset of {primary_key}. This is a partial dependency.\")\n",
    "            return True\n",
    "    \n",
    "        # Check for candidate keys\n",
    "        for candidate_key in self.candidate_keys:\n",
    "            print(f\"Checking if {determinant_attrs} is a partial dependency of candidate key {candidate_key}\")\n",
    "            if set(determinant_attrs).issubset(set(candidate_key)) and set(determinant_attrs) != set(candidate_key):\n",
    "                print(f\"{determinant_attrs} is a proper subset of candidate key {candidate_key}. This is a partial dependency.\")\n",
    "                return True\n",
    "    \n",
    "        return False\n",
    "\n",
    "    def is_transitive_dependency(self, determinant_attrs, dependent_attrs, primary_key):\n",
    "        \"\"\"\n",
    "        Helper function to check if the given functional dependency is a transitive dependency.\n",
    "        A transitive dependency exists if:\n",
    "        1. The determinant is not a subset of the primary key, and\n",
    "        2. The dependent is also not part of the primary key.\n",
    "        \"\"\"\n",
    "        print(f\"Checking if {determinant_attrs} -> {dependent_attrs} is a transitive dependency with respect to primary key {primary_key}\")\n",
    "        if not set(determinant_attrs).issubset(set(primary_key)) and not set(dependent_attrs).issubset(set(primary_key)):\n",
    "            print(f\"{determinant_attrs} -> {dependent_attrs} is a transitive dependency.\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def generate_sql(self):\n",
    "        \"\"\"\n",
    "        Generates optimized SQL statements for creating normalized tables.\n",
    "        \"\"\"\n",
    "        print(\"Generating optimized SQL for normalized tables...\")\n",
    "        for table_name, table_info in self.normalized_tables.items():\n",
    "            table_data = table_info['data']\n",
    "            primary_key = table_info['primary_key']\n",
    "\n",
    "            # Automatically determine appropriate SQL data types based on data analysis\n",
    "            columns = []\n",
    "            for col in table_data.columns:\n",
    "                col_data = table_data[col]\n",
    "                col_dtype = col_data.dtype\n",
    "                \n",
    "                if pd.api.types.is_integer_dtype(col_dtype):\n",
    "                    data_type = \"INT\"\n",
    "                elif pd.api.types.is_float_dtype(col_dtype):\n",
    "                    data_type = \"FLOAT\"\n",
    "                elif pd.api.types.is_datetime64_any_dtype(col_dtype):\n",
    "                    data_type = \"DATETIME\"\n",
    "                elif pd.api.types.is_bool_dtype(col_dtype):\n",
    "                    data_type = \"BOOLEAN\"\n",
    "                else:\n",
    "                    max_length = col_data.dropna().astype(str).str.len().max()\n",
    "                    data_type = f\"VARCHAR({max_length if max_length is not None else 255})\"\n",
    "                \n",
    "                columns.append(f\"{col} {data_type}\")\n",
    "\n",
    "            primary_key_str = ', '.join(primary_key) if primary_key else None\n",
    "            if primary_key_str:\n",
    "                create_table_sql = f\"CREATE TABLE {table_name} ({', '.join(columns)}, PRIMARY KEY ({primary_key_str}));\"\n",
    "            else:\n",
    "                create_table_sql = f\"CREATE TABLE {table_name} ({', '.join(columns)});\"\n",
    "            \n",
    "            print(create_table_sql)\n",
    "            with open('output.txt', 'a') as f:\n",
    "                f.write(create_table_sql + '\\n')\n",
    "                \n",
    "    def user_input(self):\n",
    "        # Step 1: Determine the highest normal form of the given input table\n",
    "        print(\"\\nDetermine the highest normal form of the input table.\")\n",
    "        choice = input(\"Would you like to determine the highest normal form of the given input table? (yes/no): \").lower()\n",
    "        \n",
    "        if choice == 'yes':\n",
    "            highest_nf = self.determine_highest_normal_form(self.data, self.primary_keys, self.functional_dependencies)\n",
    "            print(f\"\\nThe input table currently satisfies up to: {highest_nf}\")\n",
    "    \n",
    "        # Step 2: Ask user to proceed with normalization\n",
    "        while True:\n",
    "            # print(\"\\nChoose the highest normal form to normalize the data: 1.1NF 2.NF 3.3NF 4.BCNF 5.4NF 6.5NF\")\n",
    "            choice = input(\"Choose the highest normal form to normalize the data: 1.1NF 2.NF 3.3NF 4.BCNF 5.4NF 6.5NF\\n\")\n",
    "    \n",
    "            # Perform normalization based on the user's choice\n",
    "            if choice == '1':\n",
    "                self.normalize_1nf()\n",
    "            elif choice == '2':\n",
    "                self.normalize_1nf()\n",
    "                self.normalize_2nf()\n",
    "            elif choice == '3':\n",
    "                self.normalize_1nf()\n",
    "                self.normalize_2nf()\n",
    "                self.normalize_3nf()\n",
    "            elif choice == '4':\n",
    "                self.normalize_1nf()\n",
    "                self.normalize_2nf()\n",
    "                self.normalize_3nf()\n",
    "                self.normalize_bcnf()\n",
    "            elif choice == '5':\n",
    "                self.normalize_1nf()\n",
    "                self.normalize_2nf()\n",
    "                self.normalize_3nf()\n",
    "                self.normalize_bcnf()\n",
    "                self.normalize_4nf()\n",
    "            elif choice == '6':\n",
    "                self.normalize_1nf()\n",
    "                self.normalize_2nf()\n",
    "                self.normalize_3nf()\n",
    "                self.normalize_bcnf()\n",
    "                self.normalize_4nf()\n",
    "                self.normalize_5nf()\n",
    "            else:\n",
    "                print(\"Invalid choice. Please try again.\")\n",
    "                continue\n",
    "    \n",
    "            break\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    # Prompt user for the primary and candidate keys directly\n",
    "    primary_keys = [key.strip() for key in input(\"Enter the primary keys (comma-separated): \").split(',')]\n",
    "    candidate_keys = [key.strip() for key in input(\"Enter the candidate keys (comma-separated, or press Enter if none): \").split(',')] if input(\"Are there any candidate keys? (yes/no): \").lower() == 'yes' else []\n",
    "\n",
    "    normalizer = RDBMSNormalizer(\n",
    "        csv_file='inputTable2.csv',\n",
    "        fd_file='fd2.txt'\n",
    "    )\n",
    "    normalizer.primary_keys = primary_keys\n",
    "    normalizer.candidate_keys = candidate_keys\n",
    "    normalizer.user_input()\n",
    "    normalizer.generate_sql()\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Determine the highest normal form of the input table.\n",
      "\n",
      "Checking normal forms for the input table:\n",
      "Primary key: ['OrderID', 'DrinkID', 'FoodID', 'DrinkAllergen']\n",
      "Dependencies: ['OrderID -->> DrinkID', 'OrderID -->> FoodID']\n",
      "Non-trivial multivalued dependency detected: OrderID -->> DrinkID\n",
      "\n",
      "The input table currently satisfies up to: BCNF\n",
      "Normalizing to 1NF\n",
      "Normalizing to 2NF...\n",
      "\n",
      "Processing table: BaseTable_1NF\n",
      "Primary key for BaseTable_1NF: ['OrderID', 'DrinkID', 'FoodID', 'DrinkAllergen']\n",
      "Functional dependencies for BaseTable_1NF: []\n",
      "Table BaseTable_1NF is already in 2NF.\n",
      "Normalizing to 3NF...\n",
      "\n",
      "Processing table: BaseTable_1NF\n",
      "Primary key for BaseTable_1NF: ['OrderID', 'DrinkID', 'FoodID', 'DrinkAllergen']\n",
      "Functional dependencies for BaseTable_1NF: []\n",
      "Table BaseTable_1NF is already in 3NF.\n",
      "Normalizing to BCNF...\n",
      "\n",
      "Processing table: BaseTable_1NF\n",
      "Primary key for BaseTable_1NF: ['OrderID', 'DrinkID', 'FoodID', 'DrinkAllergen']\n",
      "Functional dependencies for BaseTable_1NF: []\n",
      "Table BaseTable_1NF is already in BCNF.\n",
      "Normalizing to 4NF...\n",
      "Decomposing table BaseTable based on MVD: ['OrderID'] -->> ['DrinkID']\n",
      "Generating optimized SQL for normalized tables...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_23748\\3427613370.py\u001B[0m in \u001B[0;36m?\u001B[1;34m()\u001B[0m\n\u001B[0;32m    945\u001B[0m     )\n\u001B[0;32m    946\u001B[0m     \u001B[0mnormalizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprimary_keys\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprimary_keys\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    947\u001B[0m     \u001B[0mnormalizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcandidate_keys\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcandidate_keys\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    948\u001B[0m     \u001B[0mnormalizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0muser_input\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 949\u001B[1;33m     \u001B[0mnormalizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgenerate_sql\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    950\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_23748\\3427613370.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    864\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    865\u001B[0m             \u001B[0mcolumns\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    866\u001B[0m             \u001B[1;32mfor\u001B[0m \u001B[0mcol\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtable_data\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    867\u001B[0m                 \u001B[0mcol_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtable_data\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mcol\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 868\u001B[1;33m                 \u001B[0mcol_dtype\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcol_data\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    869\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    870\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapi\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtypes\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_integer_dtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcol_dtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    871\u001B[0m                     \u001B[0mdata_type\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"INT\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\RDMSNormalizer\\Lib\\site-packages\\pandas\\core\\generic.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   6295\u001B[0m             \u001B[1;32mand\u001B[0m \u001B[0mname\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_accessors\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6296\u001B[0m             \u001B[1;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_info_axis\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_can_hold_identifiers_and_holds_name\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6297\u001B[0m         ):\n\u001B[0;32m   6298\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 6299\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mobject\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__getattribute__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'dtype'"
     ]
    }
   ],
   "execution_count": 167
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T02:48:54.869799Z",
     "start_time": "2024-11-02T02:48:54.866928Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1037e15a4c804771",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7a0ab2236f78b66c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
